{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":11230607,"sourceType":"datasetVersion","datasetId":7014370},{"sourceId":231637680,"sourceType":"kernelVersion"},{"sourceId":231637714,"sourceType":"kernelVersion"},{"sourceId":231637749,"sourceType":"kernelVersion"},{"sourceId":231917345,"sourceType":"kernelVersion"},{"sourceId":231919205,"sourceType":"kernelVersion"},{"sourceId":232139200,"sourceType":"kernelVersion"}],"dockerImageVersionId":30920,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":30863.394804,"end_time":"2025-04-05T03:38:52.633090","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-04-04T19:04:29.238286","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!cp /kaggle/input/nn-utils/* .","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-04-05T22:39:59.287697Z","iopub.execute_input":"2025-04-05T22:39:59.288012Z","iopub.status.idle":"2025-04-05T22:39:59.458288Z","shell.execute_reply.started":"2025-04-05T22:39:59.287974Z","shell.execute_reply":"2025-04-05T22:39:59.457161Z"},"papermill":{"duration":0.167678,"end_time":"2025-04-04T19:04:32.397231","exception":false,"start_time":"2025-04-04T19:04:32.229553","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import threading\nimport queue\nimport time\nstart_time = time.time()\nimport os\nfrom MultiTimeframeCandleManager import *\nfrom datetime import datetime, timedelta\nfrom collections import deque\nimport numpy as np\nfrom tqdm import tqdm\nimport copy\nimport tensorflow as tf\nimport random\nfrom save_and_load import *\nfrom Candle import Candle\nimport matplotlib.pyplot as plt\n#from tensorflow.keras import mixed_precision\n#mixed_precision.set_global_policy('mixed_float16')\n\nstart_time = time.time()","metadata":{"execution":{"iopub.status.busy":"2025-04-05T22:39:59.459059Z","iopub.execute_input":"2025-04-05T22:39:59.459325Z","iopub.status.idle":"2025-04-05T22:40:22.479759Z","shell.execute_reply.started":"2025-04-05T22:39:59.459297Z","shell.execute_reply":"2025-04-05T22:40:22.478560Z"},"papermill":{"duration":24.168943,"end_time":"2025-04-04T19:04:56.569056","exception":false,"start_time":"2025-04-04T19:04:32.400113","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ntrain_data = []\n\ndata_dirs = [\n    \"/kaggle/input/nn-v1-dataprep-nq2\",\n    \"/kaggle/input/nn-v1-dataprep-es2\",\n    \"/kaggle/input/nn-v1-dataprep-ym2\",\n    \"/kaggle/input/nn-v1-dataprep-eu2\",\n    \"/kaggle/input/nn-v1-dataprep-gb2\",\n]\ndata_files= []\nfor d in data_dirs:\n    subfiles = os.listdir(d)\n    for f in subfiles:\n        if \"_train_\" in f:\n            data_files.append(os.path.join(d,f))\n\n\ndata_index = 0\ndef load_new_data():\n    global data_index\n    data_index+=1\n    if data_index >= len(data_files):\n        data_index = 0\n\n    file = data_files[data_index]\n    data = obj_load(file)\n    return data\n    \n\ndata_queue = queue.Queue(maxsize=1)\n\ndef data_loader():\n    \"\"\"Background thread that continuously loads data into the queue.\"\"\"\n    while True:\n        # Only load new data if there is room in the queue.\n        if data_queue.qsize() < data_queue.maxsize:\n            new_data = load_new_data()\n            print(\"new data loaded!\")\n            data_queue.put(new_data)\n        else:\n            # Sleep briefly to avoid busy waiting.\n            time.sleep(1)\n\n# Start the data loader thread as a daemon so it exits when the main program ends.\nif False:\n    loader_thread = threading.Thread(target=data_loader, daemon=True)\n    loader_thread.daemon = True\n    loader_thread.start()\n\n# or load all data at once if it fits in memory\nif True:\n    train_data=[]\n    for d in data_dirs:\n        subfiles = os.listdir(d)\n        for f in subfiles:\n            if \"_train_\" in f:\n                    data = obj_load(os.path.join(d,f))\n                    train_data.extend(data)\n                    #break\n        #break\n\n   # info about class distribution\n    c0 = 0\n    c1 = 0\n    c2 = 0\n    print(\"info about y distribution:\")\n    for x,y in train_data:\n            if y == -1:\n                c0+=1\n            if y == 1:\n                c1+=1\n            if y == 0:\n                c2+=1\n    print(\"raw distribution:\", c0,c1,c2)\n    \n    l = len(train_data)\n    print(\"relative distribution:\", c0/l,c1/l,c2/l)","metadata":{"execution":{"iopub.status.busy":"2025-04-05T22:40:22.480597Z","iopub.execute_input":"2025-04-05T22:40:22.481032Z","iopub.status.idle":"2025-04-05T22:41:00.014614Z","shell.execute_reply.started":"2025-04-05T22:40:22.481005Z","shell.execute_reply":"2025-04-05T22:41:00.011133Z"},"papermill":{"duration":554.935438,"end_time":"2025-04-04T19:14:11.507277","exception":false,"start_time":"2025-04-04T19:04:56.571839","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if True:\n    try:\n        cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"local\")\n        tf.config.experimental_connect_to_cluster(cluster_resolver)\n        tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n        strategy = tf.distribute.TPUStrategy(cluster_resolver)\n        print(\"use tpu strategy\")\n    except:\n        strategy = tf.distribute.MirroredStrategy()\n    strategy","metadata":{"execution":{"iopub.status.busy":"2025-04-05T22:41:00.032075Z","iopub.execute_input":"2025-04-05T22:41:00.032326Z","iopub.status.idle":"2025-04-05T22:41:09.809534Z","shell.execute_reply.started":"2025-04-05T22:41:00.032303Z","shell.execute_reply":"2025-04-05T22:41:09.808613Z"},"papermill":{"duration":26.649015,"end_time":"2025-04-04T19:14:38.160234","exception":false,"start_time":"2025-04-04T19:14:11.511219","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with strategy.scope():\n#if True:\n        \n    lrelu = tf.keras.layers.LeakyReLU(0.05)\n    \n    \n    chart_m15 = tf.keras.layers.Input(shape = (60,4))\n    chart_m15 = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(x, -1000.0, 1000.0))(chart_m15)\n\n    chart_m5 = tf.keras.layers.Input(shape = (60,4))\n    chart_m5 = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(x, -1000.0, 1000.0))(chart_m5)\n    \n    chart_m1 = tf.keras.layers.Input(shape = (60,4))\n    chart_m1 = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(x, -1000.0, 1000.0))(chart_m1)\n    \n    pdas = tf.keras.layers.Input(shape = (3*3+3*3+1+12*5+5*3,))\n    pdas = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(x, -1000.0, 1000.0))(pdas)\n\n    pdas_repeated = tf.keras.layers.Lambda(\n    lambda inputs: tf.repeat(tf.expand_dims(inputs, axis = 1), repeats=60, axis=1)\n    )(pdas)\n    \n    concatenated_m15_at = tf.keras.layers.Concatenate(axis=-1)([chart_m15, pdas_repeated])\n    m15_at = tf.keras.layers.Dense(512)(concatenated_m15_at)\n    m15_at = lrelu(m15_at)\n    m15_at = tf.keras.layers.Dense(512)(m15_at)\n    m15_at = lrelu(m15_at)\n    m15_at = tf.keras.layers.Dense(256)(m15_at)\n    #m15_at = tf.keras.layers.LayerNormalization()(m15_at)\n    m15_at = lrelu(m15_at)\n    m15_at = tf.keras.layers.LSTM(256, return_sequences=False)(m15_at)\n    \n    concatenated_m5_at = tf.keras.layers.Concatenate(axis=-1)([chart_m5, pdas_repeated])\n    m5_at = tf.keras.layers.Dense(512)(concatenated_m5_at)\n    m5_at = lrelu(m5_at)\n    m5_at = tf.keras.layers.Dense(512)(m5_at)\n    m5_at = lrelu(m5_at)\n    m5_at = tf.keras.layers.Dense(256)(m5_at)\n    #m5_at = tf.keras.layers.LayerNormalization()(m5_at)\n    m5_at = lrelu(m5_at)\n    m5_at = tf.keras.layers.LSTM(256, return_sequences=False)(m5_at)\n    \n    concatenated_m1_at = tf.keras.layers.Concatenate(axis=-1)([chart_m1, pdas_repeated])\n    m1_at = tf.keras.layers.Dense(512)(concatenated_m1_at)\n    m1_at = lrelu(m1_at)\n    m1_at = tf.keras.layers.Dense(512)(m1_at)\n    m1_at = lrelu(m1_at)\n    m1_at = tf.keras.layers.Dense(256)(m1_at)\n    #m1_at = tf.keras.layers.LayerNormalization()(m1_at)\n    m1_at = lrelu(m1_at)\n    m1_at = tf.keras.layers.LSTM(256, return_sequences=False)(m1_at)\n    \n    minutes = tf.keras.layers.Input(shape = (1,))\n    minutes_embed = tf.keras.layers.Embedding(input_dim=60*24, output_dim=8)(minutes)\n    minutes_embed_flat = tf.keras.layers.Flatten()(minutes_embed)\n    \n    f15 = tf.keras.layers.Flatten()(chart_m15)\n    f5 = tf.keras.layers.Flatten()(chart_m5)\n    f1 = tf.keras.layers.Flatten()(chart_m1)\n    \n    #c = tf.keras.layers.Concatenate()([f15, f5, f1, pdas, minutes_embed_flat, current_position, scaled_open_profit])\n    c = tf.keras.layers.Concatenate()([f15, f5, f1, pdas, minutes_embed_flat, m1_at, m5_at, m15_at])\n    #c = tf.keras.layers.Concatenate()([f1, pdas, minutes_embed_flat, m1_at, m5_at, m15_at])\n    \n    \n    d = tf.keras.layers.Dense(4096*4)(c)\n    d = lrelu(d)\n    d = tf.keras.layers.Dropout(0.05)(d)\n    \n    d = tf.keras.layers.Dense(4096*4)(d)\n    d = lrelu(d)\n    d = tf.keras.layers.Dropout(0.05)(d)\n    \n    d = tf.keras.layers.Dense(4096*4)(d)\n    d = lrelu(d)\n    d = tf.keras.layers.Dropout(0.05)(d)\n    \n    d = tf.keras.layers.Dense(4096*4)(d)\n    d = lrelu(d)\n    d = tf.keras.layers.Dropout(0.05)(d)\n    \n    d = tf.keras.layers.Dense(4096*2)(d)\n    d = lrelu(d)\n    d = tf.keras.layers.Dropout(0.05)(d)\n    \n    d = tf.keras.layers.Dense(4096*1)(d)\n    d = lrelu(d)\n    d = tf.keras.layers.Dropout(0.05)(d)\n    \n    \n    \n    \n    output = tf.keras.layers.Dense(3, activation=\"softmax\", dtype=\"float32\")(d)\n    \n    model = tf.keras.Model(inputs = [chart_m15, chart_m5, chart_m1, pdas, minutes], outputs = output)\n    \n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001, clipnorm=1.0)\n    \n    \nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2025-04-05T22:41:09.810361Z","iopub.execute_input":"2025-04-05T22:41:09.810567Z","iopub.status.idle":"2025-04-05T22:41:38.179036Z","shell.execute_reply.started":"2025-04-05T22:41:09.810545Z","shell.execute_reply":"2025-04-05T22:41:38.178096Z"},"papermill":{"duration":23.364014,"end_time":"2025-04-04T19:15:01.529273","exception":false,"start_time":"2025-04-04T19:14:38.165259","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    model.load_weights(\"/kaggle/input/nn-train-v3/model.weights.h5\")\nexcept Exception as e:\n    print(e)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T22:41:38.180199Z","iopub.execute_input":"2025-04-05T22:41:38.180552Z","iopub.status.idle":"2025-04-05T22:41:38.183511Z","shell.execute_reply.started":"2025-04-05T22:41:38.180530Z","shell.execute_reply":"2025-04-05T22:41:38.182626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n\n \nclass_counts = np.array([c0, c1, c2])\nclass_weights = class_counts.sum() / (len(class_counts) * class_counts)\nclass_weights_tensor = tf.constant(class_weights, dtype=tf.float32)\nprint(\"weights:\", class_weights_tensor)\n\n\ntf.function(jit_compile=True) # my gpu does not support this\ndef tstep(data):\n    x, y = data\n\n    with tf.GradientTape() as t:\n        model_return = model(x, training=True)\n        #print(y, model_return)\n        loss = loss_fn(y, model_return)\n        #print(loss)\n        \n\n        # Apply class weights\n        sample_weights = tf.reduce_sum(y * class_weights_tensor, axis=-1)  # Select the correct weight for each sample\n        loss = loss * sample_weights  # Multiply loss by sample weights\n\n        loss = tf.reduce_mean(loss)\n\n        \n        #if loss > 10:\n        #    loss *= 0.1  # Scale down the loss if it's greater than 10, probably data point error\n\n    predicted_class = tf.argmax(model_return, axis=-1)\n    true_class = tf.argmax(y, axis=-1)\n    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_class, true_class), tf.float32))\n            \n    \n    gradient = t.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradient, model.trainable_variables))\n\n    return loss, accuracy\n\n\nbatch_size = 512\n\ndef get_data(_):\n    global train_data\n\n    while len(train_data) < 10000:\n        if not data_queue.empty():\n            new_data = data_queue.get(timeout=1)  # wait up to 1 second for data\n            train_data.extend(new_data)\n        else:\n            # If no new data is available, break out of the loop.\n            print(\"waiting for data....\")\n            time.sleep(1)\n            continue\n    \n    # Pop a batch of data from the front.\n    train_sample = train_data[:batch_size]\n    train_data = train_data[batch_size:]\n\n    # Extract multiple input features\n    states = [x[0] for x in train_sample]  # Extract input features (list of lists)\n    states_array = [\n        np.array([sample[i] for sample in states], dtype=\"float32\") for i in range(len(states[0]))\n    ]  # Convert each input feature to an array\n\n    # Convert to TensorFlow tensors\n    states_tensor = [tf.convert_to_tensor(arr) for arr in states_array]\n\n    # Extract targets and convert to tensor\n    targets = []\n    for x in train_sample:\n      v = x[1]\n      if v == 1:\n        targets.append([0,1,0])\n      elif v == -1:\n        targets.append([1,0,0])\n      else:\n        targets.append([0,0,1])\n    \n    targets_tensor = tf.convert_to_tensor(np.array(targets, dtype=\"float32\"))\n\n    return states_tensor, targets_tensor  # Return tuple (list of tensors, labels tensor)\n\n\n\n\n\ntrain_index = 0\ndef get_data_2(_):\n    global train_index\n    train_sample = [train_data[i] for i in range(train_index, train_index + batch_size)]\n    train_index += batch_size\n    if train_index + batch_size >= len(train_data):\n        train_index = 0\n\n    # Extract multiple input features\n    states = [x[0] for x in train_sample]  # Extract input features (list of lists)\n    states_array = [\n        np.array([sample[i] for sample in states], dtype=\"float32\") for i in range(len(states[0]))\n    ]  # Convert each input feature to an array\n\n    # Convert to TensorFlow tensors\n    states_tensor = [tf.convert_to_tensor(arr) for arr in states_array]\n\n    # Extract targets and convert to tensor\n    targets = []\n    for x in train_sample:\n      v = x[1]\n      if v == 1:\n        targets.append([0,1,0])\n      elif v == -1:\n        targets.append([1,0,0])\n      else:\n        targets.append([0,0,1])\n    targets_tensor = tf.convert_to_tensor(np.array(targets, dtype=\"float32\"))\n\n    return states_tensor, targets_tensor  # Return tuple (list of tensors, labels tensor)\n","metadata":{"execution":{"iopub.status.busy":"2025-04-05T22:42:42.148885Z","iopub.execute_input":"2025-04-05T22:42:42.149231Z","iopub.status.idle":"2025-04-05T22:42:42.166382Z","shell.execute_reply.started":"2025-04-05T22:42:42.149205Z","shell.execute_reply":"2025-04-05T22:42:42.165279Z"},"papermill":{"duration":0.025853,"end_time":"2025-04-04T19:15:01.561083","exception":false,"start_time":"2025-04-04T19:15:01.535230","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@tf.function(reduce_retracing=True)\ndef run(data):\n    return strategy.reduce(tf.distribute.ReduceOp.MEAN, strategy.run(tstep, args = (data,)), axis = None)\n\nimport IPython\n\nloss_lt = []\nacc_lt = []\n\nfor n in range(500):\n    losses_st = []\n    acc_st = []\n    #for _ in tqdm(range(1000)):\n    for _ in range(1000):\n        #distributed_data = (strategy.experimental_distribute_values_from_function(get_data))\n        distributed_data = (strategy.experimental_distribute_values_from_function(get_data_2))\n        loss, acc = run(distributed_data)\n        \n        #data = get_data(None)\n        #loss = tstep(data)\n        losses_st.append(loss)\n        acc_st.append(acc)\n    \n    loss_lt.append(np.mean(losses_st))\n    acc_lt.append(np.mean(acc_st))\n\n    \n    IPython.display.clear_output()\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Plot Loss\n    ax1.plot(loss_lt, label=\"Loss\", color=\"red\")\n    ax1.set_title(\"Loss\")\n    ax1.set_xlabel(\"Epochs\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.legend()\n\n    # Plot Accuracy\n    ax2.plot(acc_lt, label=\"Accuracy\", color=\"blue\")\n    ax2.set_title(\"Accuracy\")\n    ax2.set_xlabel(\"Epochs\")\n    ax2.set_ylabel(\"Accuracy\")\n    ax2.legend()\n\n    plt.show()\n    #print(loss_lt, acc_lt)\n    print(n, loss_lt[-1], acc_lt[-1])\n\n    if time.time() - start_time > 60*60*8.7:\n        break\n","metadata":{"execution":{"iopub.status.busy":"2025-04-05T22:42:42.346545Z","iopub.execute_input":"2025-04-05T22:42:42.346867Z","iopub.status.idle":"2025-04-05T22:43:00.915491Z","shell.execute_reply.started":"2025-04-05T22:42:42.346840Z","shell.execute_reply":"2025-04-05T22:43:00.914287Z"},"papermill":{"duration":30214.436214,"end_time":"2025-04-05T03:38:36.003220","exception":false,"start_time":"2025-04-04T19:15:01.567006","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_weights(\"model.weights.h5\")","metadata":{"execution":{"iopub.status.busy":"2025-04-05T22:42:24.169827Z","iopub.status.idle":"2025-04-05T22:42:24.170336Z","shell.execute_reply.started":"2025-04-05T22:42:24.170008Z","shell.execute_reply":"2025-04-05T22:42:24.170046Z"},"papermill":{"duration":6.180062,"end_time":"2025-04-05T03:38:42.191230","exception":false,"start_time":"2025-04-05T03:38:36.011168","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.00728,"end_time":"2025-04-05T03:38:42.206952","exception":false,"start_time":"2025-04-05T03:38:42.199672","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#model.load_weights(\"model.weights.h5\")","metadata":{"execution":{"iopub.status.busy":"2025-04-05T22:42:24.170931Z","iopub.status.idle":"2025-04-05T22:42:24.171558Z","shell.execute_reply.started":"2025-04-05T22:42:24.171063Z","shell.execute_reply":"2025-04-05T22:42:24.171101Z"},"papermill":{"duration":0.013799,"end_time":"2025-04-05T03:38:42.228040","exception":false,"start_time":"2025-04-05T03:38:42.214241","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.006833,"end_time":"2025-04-05T03:38:42.242380","exception":false,"start_time":"2025-04-05T03:38:42.235547","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.006974,"end_time":"2025-04-05T03:38:42.256417","exception":false,"start_time":"2025-04-05T03:38:42.249443","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.007035,"end_time":"2025-04-05T03:38:42.270399","exception":false,"start_time":"2025-04-05T03:38:42.263364","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}