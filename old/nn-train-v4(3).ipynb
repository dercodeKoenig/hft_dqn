{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":11230607,"sourceType":"datasetVersion","datasetId":7014370},{"sourceId":231637680,"sourceType":"kernelVersion"},{"sourceId":231637714,"sourceType":"kernelVersion"},{"sourceId":231637749,"sourceType":"kernelVersion"},{"sourceId":231917345,"sourceType":"kernelVersion"},{"sourceId":231919205,"sourceType":"kernelVersion"},{"sourceId":232047113,"sourceType":"kernelVersion"}],"dockerImageVersionId":30920,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":14016.637225,"end_time":"2025-04-03T22:47:43.474310","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-04-03T18:54:06.837085","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!cp /kaggle/input/nn-utils/* .","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-04-04T19:04:55.055186Z","iopub.execute_input":"2025-04-04T19:04:55.055383Z","iopub.status.idle":"2025-04-04T19:04:55.235956Z","shell.execute_reply.started":"2025-04-04T19:04:55.055361Z","shell.execute_reply":"2025-04-04T19:04:55.234815Z"},"papermill":{"duration":0.171749,"end_time":"2025-04-03T18:54:10.026268","exception":false,"start_time":"2025-04-03T18:54:09.854519","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import threading\nimport queue\nimport time\nstart_time = time.time()\nimport os\nfrom MultiTimeframeCandleManager import *\nfrom datetime import datetime, timedelta\nfrom collections import deque\nimport numpy as np\nfrom tqdm import tqdm\nimport copy\nimport tensorflow as tf\nimport random\nfrom save_and_load import *\nfrom Candle import Candle\nimport matplotlib.pyplot as plt\n#from tensorflow.keras import mixed_precision\n#mixed_precision.set_global_policy('mixed_float16')\n\nstart_time = time.time()","metadata":{"execution":{"iopub.status.busy":"2025-04-04T19:04:55.236716Z","iopub.execute_input":"2025-04-04T19:04:55.236929Z","iopub.status.idle":"2025-04-04T19:05:16.227671Z","shell.execute_reply.started":"2025-04-04T19:04:55.236905Z","shell.execute_reply":"2025-04-04T19:05:16.226103Z"},"papermill":{"duration":32.737308,"end_time":"2025-04-03T18:54:42.766333","exception":false,"start_time":"2025-04-03T18:54:10.029025","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ntrain_data = []\n\ndata_dirs = [\n    \"/kaggle/input/nn-v1-dataprep-nq2\",\n    \"/kaggle/input/nn-v1-dataprep-es2\",\n    \"/kaggle/input/nn-v1-dataprep-ym2\",\n    \"/kaggle/input/nn-v1-dataprep-eu2\",\n    \"/kaggle/input/nn-v1-dataprep-gb2\",\n]\ndata_files= []\nfor d in data_dirs:\n    subfiles = os.listdir(d)\n    for f in subfiles:\n        if \"_train_\" in f:\n            data_files.append(os.path.join(d,f))\n\n\ndata_index = 0\ndef load_new_data():\n    global data_index\n    data_index+=1\n    if data_index >= len(data_files):\n        data_index = 0\n\n    file = data_files[data_index]\n    data = obj_load(file)\n    return data\n    \n\ndata_queue = queue.Queue(maxsize=1)\n\ndef data_loader():\n    \"\"\"Background thread that continuously loads data into the queue.\"\"\"\n    while True:\n        # Only load new data if there is room in the queue.\n        if data_queue.qsize() < data_queue.maxsize:\n            new_data = load_new_data()\n            print(\"new data loaded!\")\n            data_queue.put(new_data)\n        else:\n            # Sleep briefly to avoid busy waiting.\n            time.sleep(1)\n\n# Start the data loader thread as a daemon so it exits when the main program ends.\nif False:\n    loader_thread = threading.Thread(target=data_loader, daemon=True)\n    loader_thread.daemon = True\n    loader_thread.start()\n\n# or load all data at once if it fits in memory\nif True:\n    train_data=[]\n    for d in data_dirs:\n        subfiles = os.listdir(d)\n        for f in subfiles:\n            if \"_train_\" in f:\n                    data = obj_load(os.path.join(d,f))\n                    train_data.extend(data)\n                    #break\n        #break\n\n   # info about class distribution\n    c0 = 0\n    c1 = 0\n    c2 = 0\n    print(\"info about y distribution:\")\n    for x,y in train_data:\n            if y == -1:\n                c0+=1\n            if y == 1:\n                c1+=1\n            if y == 0:\n                c2+=1\n    print(\"raw distribution:\", c0,c1,c2)\n    \n    l = len(train_data)\n    print(\"relative distribution:\", c0/l,c1/l,c2/l)","metadata":{"execution":{"iopub.status.busy":"2025-04-04T19:05:16.228882Z","iopub.execute_input":"2025-04-04T19:05:16.229405Z","iopub.status.idle":"2025-04-04T19:15:00.213841Z","shell.execute_reply.started":"2025-04-04T19:05:16.229373Z","shell.execute_reply":"2025-04-04T19:15:00.211516Z"},"papermill":{"duration":656.955177,"end_time":"2025-04-03T19:05:39.725057","exception":false,"start_time":"2025-04-03T18:54:42.769880","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if True:\n    try:\n        cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"local\")\n        tf.config.experimental_connect_to_cluster(cluster_resolver)\n        tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n        strategy = tf.distribute.TPUStrategy(cluster_resolver)\n        print(\"use tpu strategy\")\n    except:\n        strategy = tf.distribute.MirroredStrategy()\n    strategy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T19:15:00.232318Z","iopub.execute_input":"2025-04-04T19:15:00.232699Z","iopub.status.idle":"2025-04-04T19:15:28.454493Z","shell.execute_reply.started":"2025-04-04T19:15:00.232667Z","shell.execute_reply":"2025-04-04T19:15:28.453764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with strategy.scope():\n#if True:\n        \n    lrelu = tf.keras.layers.LeakyReLU(0.05)\n    \n    \n    chart_m15 = tf.keras.layers.Input(shape = (60,4))\n    chart_m15 = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(x, -1000.0, 1000.0))(chart_m15)\n\n    chart_m5 = tf.keras.layers.Input(shape = (60,4))\n    chart_m5 = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(x, -1000.0, 1000.0))(chart_m5)\n    \n    chart_m1 = tf.keras.layers.Input(shape = (60,4))\n    chart_m1 = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(x, -1000.0, 1000.0))(chart_m1)\n    \n    pdas = tf.keras.layers.Input(shape = (3*3+3*3+1+12*5+5*3,))\n    pdas = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(x, -1000.0, 1000.0))(pdas)\n\n    pdas_repeated = tf.keras.layers.Lambda(\n    lambda inputs: tf.repeat(tf.expand_dims(inputs, axis = 1), repeats=60, axis=1)\n    )(pdas)\n    \n    concatenated_m15_at = tf.keras.layers.Concatenate(axis=-1)([chart_m15, pdas_repeated])\n    m15_at = tf.keras.layers.Dense(1024)(concatenated_m15_at)\n    m15_at = lrelu(m15_at)\n    m15_at = tf.keras.layers.Dense(512)(m15_at)\n    m15_at = lrelu(m15_at)\n    m15_at = tf.keras.layers.Dense(512)(m15_at)\n    m15_at = lrelu(m15_at)\n    m15_at = tf.keras.layers.Dense(512)(m15_at)\n    #m15_at = tf.keras.layers.LayerNormalization()(m15_at)\n    m15_at = lrelu(m15_at)\n    m15_at = tf.keras.layers.LSTM(1024, return_sequences=True)(m15_at)\n    m15_at = tf.keras.layers.LSTM(1024, return_sequences=False)(m15_at)\n    \n    concatenated_m5_at = tf.keras.layers.Concatenate(axis=-1)([chart_m5, pdas_repeated])\n    m5_at = tf.keras.layers.Dense(1024)(concatenated_m5_at)\n    m5_at = lrelu(m5_at)\n    m5_at = tf.keras.layers.Dense(512)(m5_at)\n    m5_at = lrelu(m5_at)\n    m5_at = tf.keras.layers.Dense(512)(m5_at)\n    m5_at = lrelu(m5_at)\n    m5_at = tf.keras.layers.Dense(512)(m5_at)\n    #m5_at = tf.keras.layers.LayerNormalization()(m5_at)\n    m5_at = lrelu(m5_at)\n    m5_at = tf.keras.layers.LSTM(1024, return_sequences=True)(m5_at)\n    m5_at = tf.keras.layers.LSTM(1024, return_sequences=False)(m5_at)\n    \n    concatenated_m1_at = tf.keras.layers.Concatenate(axis=-1)([chart_m1, pdas_repeated])\n    m1_at = tf.keras.layers.Dense(1024)(concatenated_m1_at)\n    m1_at = lrelu(m1_at)\n    m1_at = tf.keras.layers.Dense(512)(m1_at)\n    m1_at = lrelu(m1_at)\n    m1_at = tf.keras.layers.Dense(512)(m1_at)\n    m1_at = lrelu(m1_at)\n    m1_at = tf.keras.layers.Dense(512)(m1_at)\n    #m1_at = tf.keras.layers.LayerNormalization()(m1_at)\n    m1_at = lrelu(m1_at)\n    m1_at = tf.keras.layers.LSTM(1024, return_sequences=True)(m1_at)\n    m1_at = tf.keras.layers.LSTM(1024, return_sequences=False)(m1_at)\n    \n    minutes = tf.keras.layers.Input(shape = (1,))\n    minutes_embed = tf.keras.layers.Embedding(input_dim=60*24, output_dim=8)(minutes)\n    minutes_embed_flat = tf.keras.layers.Flatten()(minutes_embed)\n    \n    f15 = tf.keras.layers.Flatten()(chart_m15)\n    f5 = tf.keras.layers.Flatten()(chart_m5)\n    f1 = tf.keras.layers.Flatten()(chart_m1)\n    \n    #c = tf.keras.layers.Concatenate()([f15, f5, f1, pdas, minutes_embed_flat, current_position, scaled_open_profit])\n    c = tf.keras.layers.Concatenate()([f15, f5, f1, pdas, minutes_embed_flat, m1_at, m5_at, m15_at])\n    #c = tf.keras.layers.Concatenate()([f1, pdas, minutes_embed_flat, m1_at, m5_at, m15_at])\n    \n    \n    d = tf.keras.layers.Dense(4096*4)(c)\n    d = lrelu(d)\n    d = tf.keras.layers.Dropout(0.05)(d)\n    \n    d = tf.keras.layers.Dense(4096*4)(d)\n    d = lrelu(d)\n    d = tf.keras.layers.Dropout(0.05)(d)\n    \n    d = tf.keras.layers.Dense(4096*2)(d)\n    d = lrelu(d)\n    d = tf.keras.layers.Dropout(0.05)(d)\n    \n    d = tf.keras.layers.Dense(4096*2)(d)\n    d = lrelu(d)\n    d = tf.keras.layers.Dropout(0.05)(d)\n    \n    \n    \n    \n    output = tf.keras.layers.Dense(3, activation=\"softmax\", dtype=\"float32\")(d)\n    \n    model = tf.keras.Model(inputs = [chart_m15, chart_m5, chart_m1, pdas, minutes], outputs = output)\n    \n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001, clipnorm=1.0)\n    \n    \nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2025-04-04T19:15:28.456196Z","iopub.execute_input":"2025-04-04T19:15:28.456483Z","iopub.status.idle":"2025-04-04T19:15:52.195834Z","shell.execute_reply.started":"2025-04-04T19:15:28.456435Z","shell.execute_reply":"2025-04-04T19:15:52.194893Z"},"papermill":{"duration":21.537466,"end_time":"2025-04-03T19:06:01.266716","exception":false,"start_time":"2025-04-03T19:05:39.729250","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    model.load_weights(\"/kaggle/input/nn-train-v4/model.weights.h5\")\nexcept Exception as e:\n    print(e)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n\n \nclass_counts = np.array([c0, c1, c2])\nclass_weights = class_counts.sum() / (len(class_counts) * class_counts)\nclass_weights_tensor = tf.constant(class_weights, dtype=tf.float32)\nprint(\"weights:\", class_weights_tensor)\n\n\ntf.function(jit_compile=True) # my gpu does not support this\ndef tstep(data):\n    x, y = data\n\n    with tf.GradientTape() as t:\n        model_return = model(x, training=True)\n        #print(y, model_return)\n        loss = loss_fn(y, model_return)\n        #print(loss)\n        \n\n        # Apply class weights\n        sample_weights = tf.reduce_sum(y * class_weights_tensor, axis=-1)  # Select the correct weight for each sample\n        loss = loss * sample_weights  # Multiply loss by sample weights\n\n        loss = tf.reduce_mean(loss)\n\n        \n        #if loss > 10:\n        #    loss *= 0.1  # Scale down the loss if it's greater than 10, probably data point error\n\n    predicted_class = tf.argmax(model_return, axis=-1)\n    true_class = tf.argmax(y, axis=-1)\n    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_class, true_class), tf.float32))\n            \n    \n    gradient = t.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradient, model.trainable_variables))\n\n    return loss, accuracy\n\n\nbatch_size = 128\n\ndef get_data(_):\n    global train_data\n\n    while len(train_data) < 10000:\n        if not data_queue.empty():\n            new_data = data_queue.get(timeout=1)  # wait up to 1 second for data\n            train_data.extend(new_data)\n        else:\n            # If no new data is available, break out of the loop.\n            print(\"waiting for data....\")\n            time.sleep(1)\n            continue\n    \n    # Pop a batch of data from the front.\n    train_sample = train_data[:batch_size]\n    train_data = train_data[batch_size:]\n\n    # Extract multiple input features\n    states = [x[0] for x in train_sample]  # Extract input features (list of lists)\n    states_array = [\n        np.array([sample[i] for sample in states], dtype=\"float32\") for i in range(len(states[0]))\n    ]  # Convert each input feature to an array\n\n    # Convert to TensorFlow tensors\n    states_tensor = [tf.convert_to_tensor(arr) for arr in states_array]\n\n    # Extract targets and convert to tensor\n    targets = []\n    for x in train_sample:\n      v = x[1]\n      if v == 1:\n        targets.append([0,1,0])\n      elif v == -1:\n        targets.append([1,0,0])\n      else:\n        targets.append([0,0,1])\n    \n    targets_tensor = tf.convert_to_tensor(np.array(targets, dtype=\"float32\"))\n\n    return states_tensor, targets_tensor  # Return tuple (list of tensors, labels tensor)\n\n\n\n\n\ntrain_index = 0\ndef get_data_2(_):\n    global train_index\n    train_sample = [train_data[i] for i in range(train_index, train_index + batch_size)]\n    train_index += batch_size\n    if train_index + batch_size >= len(train_data):\n        train_index = 0\n\n    # Extract multiple input features\n    states = [x[0] for x in train_sample]  # Extract input features (list of lists)\n    states_array = [\n        np.array([sample[i] for sample in states], dtype=\"float32\") for i in range(len(states[0]))\n    ]  # Convert each input feature to an array\n\n    # Convert to TensorFlow tensors\n    states_tensor = [tf.convert_to_tensor(arr) for arr in states_array]\n\n    # Extract targets and convert to tensor\n    targets = []\n    for x in train_sample:\n      v = x[1]\n      if v == 1:\n        targets.append([0,1,0])\n      elif v == -1:\n        targets.append([1,0,0])\n      else:\n        targets.append([0,0,1])\n    targets_tensor = tf.convert_to_tensor(np.array(targets, dtype=\"float32\"))\n\n    return states_tensor, targets_tensor  # Return tuple (list of tensors, labels tensor)\n","metadata":{"execution":{"iopub.status.busy":"2025-04-04T19:15:52.197110Z","iopub.execute_input":"2025-04-04T19:15:52.197540Z","iopub.status.idle":"2025-04-04T19:15:52.214420Z","shell.execute_reply.started":"2025-04-04T19:15:52.197510Z","shell.execute_reply":"2025-04-04T19:15:52.213289Z"},"papermill":{"duration":0.024657,"end_time":"2025-04-03T19:06:01.297424","exception":false,"start_time":"2025-04-03T19:06:01.272767","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@tf.function(reduce_retracing=True)\ndef run(data):\n    return strategy.reduce(tf.distribute.ReduceOp.MEAN, strategy.run(tstep, args = (data,)), axis = None)\n\nimport IPython\n\nloss_lt = []\nacc_lt = []\n\nfor n in range(500):\n    losses_st = []\n    acc_st = []\n    #for _ in tqdm(range(1000)):\n    for _ in range(1000):\n        #distributed_data = (strategy.experimental_distribute_values_from_function(get_data))\n        distributed_data = (strategy.experimental_distribute_values_from_function(get_data_2))\n        loss, acc = run(distributed_data)\n        \n        #data = get_data(None)\n        #loss = tstep(data)\n        losses_st.append(loss)\n        acc_st.append(acc)\n    \n    loss_lt.append(np.mean(losses_st))\n    acc_lt.append(np.mean(acc_st))\n\n    \n    IPython.display.clear_output()\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Plot Loss\n    ax1.plot(loss_lt, label=\"Loss\", color=\"red\")\n    ax1.set_title(\"Loss\")\n    ax1.set_xlabel(\"Epochs\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.legend()\n\n    # Plot Accuracy\n    ax2.plot(acc_lt, label=\"Accuracy\", color=\"blue\")\n    ax2.set_title(\"Accuracy\")\n    ax2.set_xlabel(\"Epochs\")\n    ax2.set_ylabel(\"Accuracy\")\n    ax2.legend()\n\n    plt.show()\n    #print(loss_lt, acc_lt)\n    print(n, loss_lt[-1], acc_lt[-1])\n\n    if time.time() - start_time > 60*60*8.7:\n        break\n","metadata":{"execution":{"iopub.status.busy":"2025-04-04T19:15:52.215319Z","iopub.execute_input":"2025-04-04T19:15:52.215571Z","execution_failed":"2025-04-04T23:34:28.931Z"},"papermill":{"duration":13286.505072,"end_time":"2025-04-03T22:47:27.807610","exception":false,"start_time":"2025-04-03T19:06:01.302538","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_weights(\"model.weights.h5\")","metadata":{"execution":{"execution_failed":"2025-04-04T23:34:28.934Z"},"papermill":{"duration":5.510678,"end_time":"2025-04-03T22:47:33.325578","exception":false,"start_time":"2025-04-03T22:47:27.814900","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.006978,"end_time":"2025-04-03T22:47:33.340034","exception":false,"start_time":"2025-04-03T22:47:33.333056","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#model.load_weights(\"model.weights.h5\")","metadata":{"execution":{"execution_failed":"2025-04-04T23:34:28.935Z"},"papermill":{"duration":0.013614,"end_time":"2025-04-03T22:47:33.360422","exception":false,"start_time":"2025-04-03T22:47:33.346808","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.007737,"end_time":"2025-04-03T22:47:33.375348","exception":false,"start_time":"2025-04-03T22:47:33.367611","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.007027,"end_time":"2025-04-03T22:47:33.391368","exception":false,"start_time":"2025-04-03T22:47:33.384341","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.00689,"end_time":"2025-04-03T22:47:33.405395","exception":false,"start_time":"2025-04-03T22:47:33.398505","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}