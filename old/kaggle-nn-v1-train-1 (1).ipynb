{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":11230607,"sourceType":"datasetVersion","datasetId":7014370},{"sourceId":230863568,"sourceType":"kernelVersion"},{"sourceId":230863593,"sourceType":"kernelVersion"},{"sourceId":230863616,"sourceType":"kernelVersion"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!cp /kaggle/input/nn-utils/* .","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-01T23:42:41.905653Z","iopub.execute_input":"2025-04-01T23:42:41.905955Z","iopub.status.idle":"2025-04-01T23:42:42.066293Z","shell.execute_reply.started":"2025-04-01T23:42:41.905914Z","shell.execute_reply":"2025-04-01T23:42:42.065092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import threading\nimport queue\nimport time\nstart_time = time.time()\nimport os\nfrom MultiTimeframeCandleManager import *\nfrom datetime import datetime, timedelta\nfrom collections import deque\nimport numpy as np\nfrom tqdm import tqdm\nimport copy\nimport tensorflow as tf\nimport random\nfrom save_and_load import *\nfrom Candle import Candle\nimport matplotlib.pyplot as plt\n#from tensorflow.keras import mixed_precision\n#mixed_precision.set_global_policy('mixed_float16')\n\n\n# In[6]:\n\nif True:\n    try:\n        cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"local\")\n        tf.config.experimental_connect_to_cluster(cluster_resolver)\n        tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n        strategy = tf.distribute.TPUStrategy(cluster_resolver)\n        print(\"use tpu strategy\")\n    except:\n        strategy = tf.distribute.MirroredStrategy()\n    strategy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T23:42:42.067266Z","iopub.execute_input":"2025-04-01T23:42:42.067510Z","iopub.status.idle":"2025-04-01T23:43:12.133843Z","shell.execute_reply.started":"2025-04-01T23:42:42.067485Z","shell.execute_reply":"2025-04-01T23:43:12.132097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ntrain_data = []\n\ndata_dirs = [\"/kaggle/input/nn-v1-dataprep-nq2\", \"/kaggle/input/nn-v1-dataprep-es2\", \"/kaggle/input/nn-v1-dataprep-ym2\"]\ndata_files= []\nfor d in data_dirs:\n    subfiles = os.listdir(d)\n    for f in subfiles:\n        if \"_train_\" in f:\n            data_files.append(os.path.join(d,f))\n\n\ndata_index = 0\ndef load_new_data():\n    global data_index\n    data_index+=1\n    if data_index >= len(data_files):\n        data_index = 0\n\n    file = data_files[data_index]\n    data = obj_load(file)\n    return data\n    \n\ndata_queue = queue.Queue(maxsize=1)\n\ndef data_loader():\n    \"\"\"Background thread that continuously loads data into the queue.\"\"\"\n    while True:\n        # Only load new data if there is room in the queue.\n        if data_queue.qsize() < data_queue.maxsize:\n            new_data = load_new_data()\n            print(\"new data loaded!\")\n            data_queue.put(new_data)\n        else:\n            # Sleep briefly to avoid busy waiting.\n            time.sleep(1)\n\n# Start the data loader thread as a daemon so it exits when the main program ends.\nif False:\n    loader_thread = threading.Thread(target=data_loader, daemon=True)\n    loader_thread.daemon = True\n    loader_thread.start()\n\n# or load all data at once if it fits in memory\nif True:\n    train_data=[]\n    for d in data_dirs:\n        subfiles = os.listdir(d)\n        for f in subfiles:\n            if \"_train_\" in f:\n                    data = obj_load(os.path.join(d,f))\n                    train_data.extend(data)\n\n   # info about class distribution\n    c0 = 0\n    c1 = 0\n    c2 = 0\n    print(\"info about y distribution:\")\n    for x,y in tqdm(train_data):\n            if y == -1:\n                c0+=1\n            if y == 1:\n                c1+=1\n            if y == 0:\n                c2+=1\n    print(\"raw distribution:\", c0,c1,c2)\n    \n    l = len(train_data)\n    print(\"relative distribution:\", c0/l,c1/l,c2/l)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T23:43:12.135026Z","iopub.execute_input":"2025-04-01T23:43:12.135397Z","iopub.status.idle":"2025-04-01T23:49:42.983512Z","shell.execute_reply.started":"2025-04-01T23:43:12.135348Z","shell.execute_reply":"2025-04-01T23:49:42.980348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with strategy.scope():\n#if True:\n    \n    lrelu = tf.keras.layers.LeakyReLU(0.05)\n    \n    \n    chart_m15 = tf.keras.layers.Input(shape = (60,4))\n    chart_m5 = tf.keras.layers.Input(shape = (60,4))\n    chart_m1 = tf.keras.layers.Input(shape = (60,4))\n    \n    pdas = tf.keras.layers.Input(shape = (3*3+3*3+1+12*5+5*3,))\n    \n    pdas_repeated = tf.keras.layers.Lambda(\n    lambda inputs: tf.repeat(tf.expand_dims(inputs, axis = 1), repeats=60, axis=1)\n    )(pdas)\n    \n    concatenated_m5_at = tf.keras.layers.Concatenate(axis=-1)([chart_m5, pdas_repeated])\n    m5_at = tf.keras.layers.Dense(512)(concatenated_m5_at)\n    m5_at = lrelu(m5_at)\n    m5_at = tf.keras.layers.Dense(256)(m5_at)\n    m5_at = lrelu(m5_at)\n    m5_at = tf.keras.layers.Dense(256)(m5_at)\n    m5_at = lrelu(m5_at)\n    m5_at = tf.keras.layers.LSTM(512)(m5_at)\n    \n    concatenated_m1_at = tf.keras.layers.Concatenate(axis=-1)([chart_m1, pdas_repeated])\n    m1_at = tf.keras.layers.Dense(512)(concatenated_m1_at)\n    m1_at = lrelu(m1_at)\n    m1_at = tf.keras.layers.Dense(256)(m1_at)\n    m1_at = lrelu(m1_at)\n    m1_at = tf.keras.layers.Dense(256)(m1_at)\n    m1_at = lrelu(m1_at)\n    m1_at = tf.keras.layers.LSTM(512)(m1_at)\n    \n    minutes = tf.keras.layers.Input(shape = (1,))\n    minutes_embed = tf.keras.layers.Embedding(input_dim=60*24, output_dim=8)(minutes)\n    minutes_embed_flat = tf.keras.layers.Flatten()(minutes_embed)\n    \n    f15 = tf.keras.layers.Flatten()(chart_m15)\n    f5 = tf.keras.layers.Flatten()(chart_m5)\n    f1 = tf.keras.layers.Flatten()(chart_m1)\n    \n    #c = tf.keras.layers.Concatenate()([f15, f5, f1, pdas, minutes_embed_flat, current_position, scaled_open_profit])\n    c = tf.keras.layers.Concatenate()([f15, f5, f1, pdas, minutes_embed_flat, m1_at, m5_at])\n    \n    \n    d = tf.keras.layers.Dense(1024*2)(c)\n    d = lrelu(d)\n    d = tf.keras.layers.Dropout(0.05)(d)\n    \n    d = tf.keras.layers.Dense(1024*2)(d)\n    d = lrelu(d)\n    d = tf.keras.layers.Dropout(0.05)(d)\n    \n    d = tf.keras.layers.Dense(1024*2)(d)\n    d = lrelu(d)\n    d = tf.keras.layers.Dropout(0.05)(d)\n    \n    d = tf.keras.layers.Dense(1024*2)(d)\n    d = lrelu(d)\n    d = tf.keras.layers.Dropout(0.05)(d)\n\n    \n    \n    \n    output = tf.keras.layers.Dense(3, activation=\"softmax\", dtype=\"float32\")(d)\n    \n    model = tf.keras.Model(inputs = [chart_m15, chart_m5, chart_m1, pdas, minutes], outputs = output)\n    \n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001, clipnorm=1.0)\n    \n    \nmodel.summary()\n    \n    \n    # In[10]:\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T23:51:11.458278Z","iopub.execute_input":"2025-04-01T23:51:11.458656Z","iopub.status.idle":"2025-04-01T23:51:33.109723Z","shell.execute_reply.started":"2025-04-01T23:51:11.458627Z","shell.execute_reply":"2025-04-01T23:51:33.108552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n\n \nclass_counts = np.array([c0, c1, c2])\nclass_weights = class_counts.sum() / (len(class_counts) * class_counts)\nclass_weights_tensor = tf.constant(class_weights, dtype=tf.float32)\nprint(\"weights:\", class_weights_tensor)\n\n\ntf.function(jit_compile=True) # my gpu does not support this\ndef tstep(data):\n    x, y = data\n\n    with tf.GradientTape() as t:\n        model_return = model(x, training=True)\n        #print(y, model_return)\n        loss = loss_fn(y, model_return)\n        #print(loss)\n        \n\n        # Apply class weights\n        sample_weights = tf.reduce_sum(y * class_weights_tensor, axis=-1)  # Select the correct weight for each sample\n        loss = loss * sample_weights  # Multiply loss by sample weights\n\n        loss = tf.reduce_mean(loss)\n\n        \n        if loss > 10:\n            loss *= 0.1  # Scale down the loss if it's greater than 10, probably data error\n\n    predicted_class = tf.argmax(model_return, axis=-1)\n    true_class = tf.argmax(y, axis=-1)\n    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_class, true_class), tf.float32))\n            \n    \n    gradient = t.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradient, model.trainable_variables))\n\n    return loss, accuracy\n\n\nbatch_size = 1024\n\ndef get_data(_):\n    global train_data\n\n    while len(train_data) < 10000:\n        if not data_queue.empty():\n            new_data = data_queue.get(timeout=1)  # wait up to 1 second for data\n            train_data.extend(new_data)\n        else:\n            # If no new data is available, break out of the loop.\n            print(\"waiting for data....\")\n            time.sleep(1)\n            continue\n    \n    # Pop a batch of data from the front.\n    train_sample = train_data[:batch_size]\n    train_data = train_data[batch_size:]\n\n    # Extract multiple input features\n    states = [x[0] for x in train_sample]  # Extract input features (list of lists)\n    states_array = [\n        np.array([sample[i] for sample in states], dtype=\"float32\") for i in range(len(states[0]))\n    ]  # Convert each input feature to an array\n\n    # Convert to TensorFlow tensors\n    states_tensor = [tf.convert_to_tensor(arr) for arr in states_array]\n\n    # Extract targets and convert to tensor\n    targets = []\n    for x in train_sample:\n      v = x[1]\n      if v == 1:\n        targets.append([0,1,0])\n      elif v == -1:\n        targets.append([1,0,0])\n      else:\n        targets.append([0,0,1])\n    \n    targets_tensor = tf.convert_to_tensor(np.array(targets, dtype=\"float32\"))\n\n    return states_tensor, targets_tensor  # Return tuple (list of tensors, labels tensor)\n\n\n\n\n\ntrain_index = 0\ndef get_data_2(_):\n    global train_index\n    train_sample = [train_data[i] for i in range(train_index, train_index + batch_size)]\n    train_index += batch_size\n    if train_index + batch_size >= len(train_data):\n        train_index = 0\n\n    # Extract multiple input features\n    states = [x[0] for x in train_sample]  # Extract input features (list of lists)\n    states_array = [\n        np.array([sample[i] for sample in states], dtype=\"float32\") for i in range(len(states[0]))\n    ]  # Convert each input feature to an array\n\n    # Convert to TensorFlow tensors\n    states_tensor = [tf.convert_to_tensor(arr) for arr in states_array]\n\n    # Extract targets and convert to tensor\n    targets = []\n    for x in train_sample:\n      v = x[1]\n      if v == 1:\n        targets.append([0,1,0])\n      elif v == -1:\n        targets.append([1,0,0])\n      else:\n        targets.append([0,0,1])\n    targets_tensor = tf.convert_to_tensor(np.array(targets, dtype=\"float32\"))\n\n    return states_tensor, targets_tensor  # Return tuple (list of tensors, labels tensor)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T23:51:44.783590Z","iopub.execute_input":"2025-04-01T23:51:44.784011Z","iopub.status.idle":"2025-04-01T23:51:44.803295Z","shell.execute_reply.started":"2025-04-01T23:51:44.783962Z","shell.execute_reply":"2025-04-01T23:51:44.801963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@tf.function(reduce_retracing=True)\ndef run(data):\n    return strategy.reduce(tf.distribute.ReduceOp.MEAN, strategy.run(tstep, args = (data,)), axis = None)\n\nimport IPython\n\nloss_lt = []\nacc_lt = []\n\nfor _ in range(100):\n    losses_st = []\n    acc_st = []\n    #for _ in tqdm(range(1000)):\n    for _ in range(1000):\n        #distributed_data = (strategy.experimental_distribute_values_from_function(get_data))\n        distributed_data = (strategy.experimental_distribute_values_from_function(get_data_2))\n        loss, acc = run(distributed_data)\n        \n        #data = get_data(None)\n        #loss = tstep(data)\n        losses_st.append(loss)\n        acc_st.append(acc)\n    \n    loss_lt.append(np.mean(losses_st))\n    acc_lt.append(np.mean(acc_st))\n\n    \n    IPython.display.clear_output()\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Plot Loss\n    ax1.plot(loss_lt, label=\"Loss\", color=\"red\")\n    ax1.set_title(\"Loss over Epochs\")\n    ax1.set_xlabel(\"Epochs\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.legend()\n\n    # Plot Accuracy\n    ax2.plot(acc_lt, label=\"Accuracy\", color=\"blue\")\n    ax2.set_title(\"Accuracy over Epochs\")\n    ax2.set_xlabel(\"Epochs\")\n    ax2.set_ylabel(\"Accuracy\")\n    ax2.legend()\n\n    plt.show()\n    print(loss_lt, acc_lt)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T23:52:08.610025Z","iopub.execute_input":"2025-04-01T23:52:08.610417Z","iopub.status.idle":"2025-04-02T00:28:17.707073Z","shell.execute_reply.started":"2025-04-01T23:52:08.610390Z","shell.execute_reply":"2025-04-02T00:28:17.705407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"model.keras\")\nmodel.save_weights(\"model.weights.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T00:28:23.312219Z","iopub.execute_input":"2025-04-02T00:28:23.312602Z","iopub.status.idle":"2025-04-02T00:28:23.561788Z","shell.execute_reply.started":"2025-04-02T00:28:23.312574Z","shell.execute_reply":"2025-04-02T00:28:23.560578Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"from ftplib import *\n\n\n# FTP server details\nftp_server = 'benundmarvpromotions.lima-ftp.de'\nftp_user = 'benundmarvpromotions'\nftp_password = 'defrtrn2343fdsgfcdf'\n\nmodel_save_name = \"model.keras\"\nfile_to_upload = model_save_name\nremote_path = '/test/' + model_save_name\n\n# Connect to the FTP server\nftp = FTP(ftp_server)\nftp.login(user=ftp_user, passwd=ftp_password)\n\nftp.cwd(\"/\")\n\n# Open the file in binary mode and upload it\nwith open(file_to_upload, 'rb') as file:\n    ftp.storbinary(f'STOR {remote_path}', file, blocksize = 1024*1024)\n\n# Close the connection\nftp.quit()","metadata":{"execution":{"iopub.status.busy":"2025-04-02T00:28:23.562726Z","iopub.execute_input":"2025-04-02T00:28:23.563012Z","iopub.status.idle":"2025-04-02T00:28:24.062183Z","shell.execute_reply.started":"2025-04-02T00:28:23.562985Z","shell.execute_reply":"2025-04-02T00:28:24.060855Z"}}},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T23:49:48.781355Z","iopub.status.idle":"2025-04-01T23:49:48.781876Z","shell.execute_reply.started":"2025-04-01T23:49:48.781507Z","shell.execute_reply":"2025-04-01T23:49:48.781549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#model.load_weights(\"model.weights.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T23:49:48.782484Z","iopub.status.idle":"2025-04-01T23:49:48.782922Z","shell.execute_reply.started":"2025-04-01T23:49:48.782646Z","shell.execute_reply":"2025-04-01T23:49:48.782694Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}